1. Evaluating a classifier

Imagine that you have a generator G\mathcal{G}G that pulls from a finite dataset of millions of points.

Let's assume that Dtrain\mathcal{D}_{\it train}Dtrain​ is one such output of the generator G\mathcal{G}G.

Consider the situation in which you have run a machine learning algorithm on some training dataset Dtrain\mathcal{D}_{\it train}Dtrain​, and it has returned to you a specific hhh. Your job is to design (but not implement yet!) a procedure for evaluating hhh's effectiveness as a classifier. (Want more on classifiers? Check the notes)

Assume we have a score function that takes a classifier hhh, dataset DDD - a tuple of data and labels: (X,y)(X,y)(X,y) - and returns the percentage of correctly classified examples as a decimal between 0 and 1. We'll package it as follows:

def eval_classifier(h, D):
    test_X, test_y = D
    return score(h, test_X, test_y)
                                                                                                                                                                                                                                                                                                
A) Percy Eptron suggests reusing the training data to assess hhh:

eval_classifier(h, D_train)

Explain why Percy's strategy might not be so good.

His strategy won't be effective in situations where the test data is radically different than the test data, i.e. poor generalization

B) Now write down a better approach for evaluating hhh, which may use hhh, G\mathcal{G}G, and Dtrain\mathcal{D}_{\it train}Dtrain​, and computes a score for hhh. The syntax is not important, but do write something down. What does this score measure and what is the range of possible outputs?

Assume we have a function generate which takes in G and returns a randomly sampled dataset

def eval2 (h, G, n_iter):
	D=generate(G)
	count=0
	min_score=0
	
	while (count <  n_iter):
		big=eval_classifier(h,D)
		
		if (big > min_score) then min_score=big

C) Explain why your method might be more desirable than Percy's. What problem does it fix?

This method is more desirable than our foolish friend's because it is better equipped to handle test datasets that differ in composition to the training set.
It is also customizable to the user's patience level.

D) How would your method from B score the classifier hhh, if Dtest\mathcal{D}_{\it test}Dtest​ came from a different distribution than G\mathcal{G}G, but Dtrain\mathcal{D}_{\it train}Dtrain​ was unchanged?

In this scenario my method would score the classifier inaccurately given that the underlying distribution changed, and the degree of inaccuray would change depending on how different the new dist (say F) was from G.


2) Evaluating a learning algorithm

A learning algorithm is a function LLL that takes as input

    data set Dtrain\mathcal{D}_{\it train}Dtrain​ as training data

and returns

    a classifier hhh.

A) Would running the learning algorithm LLL on two different training datasets Dtrain1\mathcal{D}_{\it train_1}Dtrain1​​ and Dtrain2\mathcal{D}_{\it train_2}Dtrain2​​ produce the same classifier? In other words, would h1h_1h1​ = L(Dtrain1)L(\mathcal{D}_{\it train_1})L(Dtrain1​​) be the same classifier as h2h_2h2​ = L(Dtrain2)L(\mathcal{D}_{\it train_2})L(Dtrain2​​)? What if those training datasets were pulled from the same distribution?

h1 would not be the same as h2 in this case. 
If Dtrain1 and Dtrain2 were sampled from the same distribution then it wouldn't be guaranteed that h1=h2 but at the very least they would perform similarly.

Now, consider a situation in which someone is trying to sell you a new learning algorithm, and you want to know how good it is. There is an interesting result that says that without any assumptions about your data, There is no learning algorithm that, for all data sources, is better than every other learning algorithm. So, you'll need to assess the learning algorithm's performance in the context of a particular data source.

Check Yourself: What is the difference between a classifier and a learning algorithm? Understanding the distinction will help you when thinking about this question. (Stuck? Check the notes)

A classifier only assigns a particular data point to one of at least two categories, while a learning algorithm is any general computational method that establishes the structure of a dataset to be used for some purpose.

Assume that you have a generator of labeled data, G\mathcal{G}G, which will be suitable for your application. The learning algorithm's performance on G\mathcal{G}G-generated data will be a good predictor of the learning algorithm's performance on data from your application. (You can review how to evaluate learning algorithms in the notes)

B) Linnea Separatorix wants to evaluate a learning algorithm, and suggests the following procedure:

def eval_learning_alg(L, G, n):
    # draw a set of n training examples (points and labels)
    train_X, train_y = G(n)
    # run L
    h = L(train_X, train_y)
    # evaluate using your classifier scoring procedure, on some new labeled data
    test_data = G(n) # draw new set of test data
    return eval_classifier(h, test_data)

Check Yourself: What are GGG and nnn in the code above?

G and n are defined in the function header, where G is the data generating function and n is the number data points used to train the algorithm.

Explain why Linnea's strategy might not be so good.

Her strategy will create problems when the test set includes data points already used in training.

C) Next, Linnea decides to generate one classifier hhh but evaluate that classifier with multiple (10) test sets in her eval_learning_alg. More specifically, Linnea changed her code above into:

def eval_learning_alg(L, G, n):
    # draw a set of n training examples (points and labels)
    train_X, train_y = G(n)
    # run L
    h = L(train_X, train_y)
    # evaluate using your classifier scoring procedure, on some new labeled data
    score = 0
    for i in range(10):
        test_data = G(n) # draw new set of test data
        score += eval_classifier(h, test_data)
    return score/10

Is Linnea's strategy good now? Explain why or why not.

This new strategy is a slight improvement but will still suffer from the same issue described above.
Additionally, the mean score is returned rather than the best.

Check Yourself: How many classifiers is Linea generating and testing from the learning algorithm?

She generates one classifier and tests it 10 times.

D) Now design a better procedure for evaluating LLL. Write pseudocode for a procedure that takes LLL, G\mathcal{G}G and nnn and returns a score. Say what the output score measures and what the best and worst values are.

def better_eval_learning_alg(L, G, n):
    #take training samples
	trainX, trainY=G(n)
	#runnit
	h=L(trainX, trainY)
	
	#EEVALKILMER
	#Assume we have a function called sampleWithoutDuplicates which rejects a sample point if it's already in the training set
	#It takes G, n as inputs and returns n points
	
	min_score=0
	big=0
	
	for j in range(10):
		bigtest=sampleWithoutDuplicates(G,n)
		big=eval_classifier(h, bigtest)
		
		if (big > min_score) then min_score=big
		
	return min_score
	

E) Explain why your method might be more desirable than Linnea's.

It addresses the 2 problems described in C) - UNDERSTAND!? Fig fuck

3) Evaluating a learning algorithm with a small amount of data

In reality, it's almost never possible to have a generator of all the data you want; in fact, in some domains data is very expensive to collect, and so you are given a fixed, small set of samples. Now assume that you only have 100 labeled data points to use for training and testing/evaluation.

A) In the last section, you thought about how to evaluate a learning algorithm. Now that you are given only 100 labeled data points in total, how would you evaluate a learning algorithm? Specifically, how would you implement better_eval_learning_alg from 2C) without G\mathcal{G}G but instead with your 100 labeled data? (You don't need to write out new pseudocode for better_eval_learning_alg but still think about how you would implement it.) 

In this restricted/realistic scenario a better approach would be to split the given 100 points into training, test, and validation sets of approximately equal size (say 25 pts.)